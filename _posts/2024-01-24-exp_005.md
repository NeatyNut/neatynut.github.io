---
layer: single
title: "Aiffel) Exploration - ëŒ€í™”í˜• ì±—ë´‡(Transformer) ì •ë¦¬"
categories: Exploration
tag: [Transformer, chat-bot]
use_math: true
---

ë‚´ìš© ì¶œì²˜ : [Aiffel](https://www.aiffel.io/)

- ì—°í˜

	| ì—°í˜ | ë‚´ìš© |
	| ---- | ---- |
	| 1950ë…„ | ì•¨ëŸ° íŠœë§ì˜ ì´ë¯¸í…Œì´ì…˜ ê²Œì„(ê°œë… ë“±ì¥) |
	| 1966ë…„ | MIT, ELIZA ê°œë°œ(ë‹¨ìˆœ ì±—ë´‡ ë°œëª…) |
	| 2011ë…„ | ì• í”Œ, Siri ì¶œì‹œ(ì‚¬ìš©ì„± ë†’ì€ ì±—ë´‡) |
	| 2017ë…„ | ALBERT, BERT, ULMFiT, Transformer-XL<br>(ë›°ì–´ë‚œ ì„±ëŠ¥ì˜ ì±—ë´‡ ë“±ì¥) |

<br/>

- âœ¨ ì±—ë´‡ì˜ 5ê°€ì§€ ëŒ€í‘œ ìœ í˜•(ê·¸ë¦¼ì¶œì²˜ : [Tony Around](https://tonyaround.com/%ec%b1%97%eb%b4%87-%ea%b8%b0%ed%9a%8d-%eb%8b%a8%ea%b3%84-%ec%b1%97%eb%b4%87%ec%9d%98-5%ea%b0%80%ec%a7%80-%eb%8c%80%ed%91%9c-%ec%9c%a0%ed%98%95-%ec%a2%85%eb%a5%98/))
	- ëŒ€í™”í˜• ì±—ë´‡ : ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ê¸°ë°˜ ìì—°ì–´ ì²˜ë¦¬	
	- íŠ¸ë¦¬í˜•(ë²„íŠ¼) ì±—ë´‡ : ì •í•´ì§„ íŠ¸ë¦¬ì— ë”°ë¼ ë‹µë³€
	- ì¶”ì²œí˜• ì±—ë´‡ : ì‚¬ì „ ì •ì˜ëœ ë‹µë³€ë¥¼ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ìš°ì„  ìˆœìœ„ë³„ë¡œ ë¦¬ìŠ¤íŠ¸
	- ì‹œë‚˜ë¦¬ì˜¤í˜• ì±—ë´‡ : ì›í•˜ëŠ” ì„œë¹„ìŠ¤ ì œê³µì„ ìœ„í•´ ì •í•´ì§„ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìˆ˜í–‰
	- ê²°í•©í˜• ì±—ë´‡ : ë¹„ì¦ˆë‹ˆìŠ¤ ëª©ì ì— ë”°ë¼ ì—¬ëŸ¬ ì±—ë´‡ ìœ í˜•ë“¤ì„ ê²¹í•©

## 1. ì¸ì½”ë”ë””ì½”ë”
### 1) ì¸ì½”ë” êµ¬ì¡° : ë‘ ê°€ì§€ ì•„í‚¤í…ì³
![img6](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_UcFQAjh.max-800x600.png)

- í•™ìŠµ ì‹œ Paired data set(ì…ë ¥ ë°ì´í„° / ì¶œë ¥ ë°ì´í„°)ê°€ í•„ìš”

## 2. íŠ¸ëœìŠ¤í¬ë¨¸ì˜ êµ¬ì¡°
### 1) í¬ì§€ì…”ë„ ì¸ì½”ë”©
- ì´ë¯¸ì§€ ì¶œì²˜ : [http://jalammar.github.io/illustrated-transformer](http://jalammar.github.io/illustrated-transformer)
![img7](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_2_EnQyi4S.max-800x600.png)
- íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë” : ëˆ„ì í•´ ìŒ“ì•„ ì˜¬ë¦° ì¸µì„ í†µí•´ ì •ë³´ ì¶”ì¶œ
- íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë””ì½”ë” : ëˆ„ì í•´ ìŒ“ì•„ ì˜¬ë¦° ë””ì½”ë”ì˜ ì¸µì„ í†µí•´ ì¶œë ¥
	- ì´ë•Œ, í•­ìƒ ë§ˆì§€ë§‰ ì¸ì½”ë” ë ˆì´ì–´ì˜ ì¶œë ¥ì´ ëª¨ë“  ë””ì½”ë” ë ˆì´ì–´ì— ì…ë ¥ë¨

<br/>

![img8](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_3_ddZedfW.max-800x600.png)

<br/>

- `ê¸°ì¡´ ìì—°ì–´ ì²˜ë¦¬ì™€ì˜ ì°¨ì´ì  : í¬ì§€ì…”ë„ ì¸ì½”ë”©(positional Encoding)`
	![img9](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_5_kH52kQN.png)
	- ì™œ í•„ìš”í•œê°€?
		- íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥ ë°›ëŠ” RNNê³¼ ë‹¬ë¦¬ í•œêº¼ë²ˆì— ì…ë ¥ ë°›ê¸° ë•Œë¬¸ì— `ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´`, ì¦‰ `í¬ì§€ì…”ë„ ì¸ì½”ë”©`ì´ í•„ìš”í•¨.

	- ì‹¤ì œ ë…¼ë¬¸ì— ì œì‹œëœ í¬ì§€ì…”ë„ ì¸ì½”ë”©

		![img10](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_9_l58gVWT.max-800x600.png)
		
		- ì¸ì½”ë”, ë””ì½”ë” ëª¨ë‘ ì„ë² ë”© ì§í›„ì— ì¶”ê°€ë˜ëŠ” ëª¨ìŠµ

		---

		- ìˆ˜ì‹
		
		![img11](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_7_3Rneu0P.png){: width="70%" height="50%"}{: .align-center}
		<div>$$PE_{(pos, 2i)} = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})$$</div>
		<div>$$PE_{(pos, 2i+1)} = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})$$</div>

		> $pos$ : ì…ë ¥ ë¬¸ì¥ì—ì„œì˜ ì„ë² ë”© ë²¡í„°ì˜ ìœ„ì¹˜(í–‰ ìœ„ì¹˜)<br/>
		> $i$ : ì„ë² ë”© ë²¡í„° ë‚´ ì°¨ì›ì˜ ì¸ë±ìŠ¤(ì—´ ìœ„ì¹˜)<br/>
		> $d_{model}$ : ì„ë² ë”© ë²¡í„°ì˜ ì°¨


	- ì½”ë“œ êµ¬í˜„
		```python
		import tensorflow as tf
		import tensorflow_datasets as tfds
		import os
		import re
		import numpy as np
		import matplotlib.pyplot as plt

		## í´ë˜ìŠ¤ êµ¬í˜„
		# í¬ì§€ì…”ë„ ì¸ì½”ë”© ë ˆì´ì–´
		class PositionalEncoding(tf.keras.layers.Layer):
		
			def __init__(self, position, d_model):
				super(PositionalEncoding, self).__init__()
				self.pos_encoding = self.positional_encoding(position, d_model)
		
			def get_angles(self, position, i, d_model):
				angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32)) # (d_modelê°’)
				return position * angles # (positionê°’, d_modelê°’) # ë¸Œë¡œë“œìºìŠ¤íŠ¸
		
			def positional_encoding(self, position, d_model):
				# ê°ë„ ë°°ì—´ ìƒì„±
				angle_rads = self.get_angles(
					position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],
					i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],
					d_model=d_model) # position, i shapeëŠ” ê°ê° (positionê°’, 1), (1, d_modelê°’)
			
				# ë°°ì—´ì˜ ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” sin í•¨ìˆ˜ ì ìš©
				sines = tf.math.sin(angle_rads[:, 0::2]) # (positionê°’, d_modelê°’/2)
			
				# ë°°ì—´ì˜ í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” cosine í•¨ìˆ˜ ì ìš©
				cosines = tf.math.cos(angle_rads[:, 1::2]) # (positionê°’, d_modelê°’/2)
			
				# sinê³¼ cosineì´ êµì°¨ë˜ë„ë¡ ì¬ë°°ì—´
				pos_encoding = tf.stack([sines, cosines], axis=0) # (2, positionê°’, d_modelê°’/2)
				pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) # (positionê°’, d_modelê°’/2, 2)
				pos_encoding = tf.reshape(pos_encoding, [position, d_model]) # (positionê°’, d_modelê°’) ë‹¤ì‹œ êµì°¨ë¨!
			
				pos_encoding = pos_encoding[tf.newaxis, ...] # (1, positionê°’, d_modelê°’)
				return tf.cast(pos_encoding, tf.float32)
		
		def call(self, inputs):
				return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]

		## ì‹œê°í™” í•˜ê¸°
		sample_pos_encoding = PositionalEncoding(50, 512)
		plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')
		plt.xlabel('Depth')
		plt.xlim((0, 512))
		plt.ylabel('Position')
		plt.colorbar()
		plt.show()
		```
	
### 2) ì–´í…ì…˜
- ì–´í…ì…˜ ê°’ì´ë€? `ë‹¨ì–´ê°„ì˜ ìœ ì‚¬ë„(ë‹¨ì–´ê°„ì˜ ê±°ë¦¬|ìœ í´ë¦¬ë“œ)`
	
	![img12](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_10_AaCfqrY.png)
	- ì–´í…ì…˜ ê°’ ë„ì¶œ ê³¼ì •
		1. 'ì¿¼ë¦¬(Query)'ì™€ ëª¨ë“  'í‚¤(Key)'ì˜ ìœ ì‚¬ë„ë¥¼ ê°ê° ë„ì¶œ
		2. ìœ ì‚¬ë„ë¥¼ í‚¤(Key)ì™€ ë§µí•‘ ëœ ê° 'ê°’(Value)'ì— ë°˜ì˜
		3. ìœ ì‚¬ë„ê°€ ë°˜ì˜ëœ 'ê°’(Value)'ì„ ëª¨ë‘ ë”í•´ì„œ ë­‰ì³ì¤Œ

		<br/>
		- ğŸ“Œì£¼ìš” ìš©ì–´
			
			> ì¿¼ë¦¬(Query) : ê¸°ì¤€ ë‹¨ì–´ ì„ë² ë”©$(pos, d_{model})$   x   $W_{Query}$<br/>
			> í‚¤(Key) : ì¸¡ì •í•  ë‹¨ì–´ ì„ë² ë”©$(pos, d_{model})$   x   $W_{Key}$<br/>
			> ê°’(Value) : ì¸¡ì •í•  ë‹¨ì–´ ì„ë² ë”©$(pos, d_{model})$   x   $W_{Value}$
			
- ìˆ˜ì‹
	- $Attention(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V$
		- $Q$, $K$, $V$ ëŠ” ê°ê° ì¿¼ë¦¬(Query), í‚¤(Key), ê°’(Value)<br/>
<br/>
	- $QK^{T}$<br/>
		![att1](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_15_pUfIgKn.png)
		- ì´ˆë¡ìƒ‰ í–‰ë ¬ = ê° ë‹¨ì–´ ë²¡í„° ìœ ì‚¬ë„(ê±°ë¦¬)ê°€ ëª¨ë‘ ê¸°ë¡ëœ í–‰ë ¬<br/>
<br/>
	- ê·¸ë¦¼ìœ¼ë¡œ ë°”ê¾¼ ìˆ˜ì‹<br/>
		![att2](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_16_neA52rZ.png)
		- `ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜(dot product attention)`
			- Keyì˜ depth(í–‰ ê°œìˆ˜)ì¸ $\sqrt{d_{k}}$ë¡œ ìŠ¤ì¼€ì¼ë§
			- softmaxí•¨ìˆ˜ì˜ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ í•´ê²°<br/>
<br/>
- íŠ¸ëœìŠ¤í¬ë¨¸ì— ì‚¬ìš©ëœ ì–´í…ì…˜<br/>
	![img13](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_11_tFFhFjx.png)

	- 1. **ì¸ì½”ë” ì…€í”„ ì–´í…ì…˜**   `[ì¸ì½”ë”]`
	- 2. **ë””ì½”ë” ì…€í”„ ì–´í…ì…˜**   `[ë””ì½”ë”]`
	- 3. **ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜**   `[ë””ì½”ë”]`<br/>
<br/>
	- âœ¨ ì…€í”„ ì–´í…ì…˜ì´ë€?
		- ëŒ€ìƒì´ í•œ ë¬¸ì¥ ë‚´ì˜ ë‹¨ì–´ë“¤ì¸ ê²½ìš°<br/>
<br/>
	- **ğŸ“Œì˜ˆì‹œ : ì €ëŠ” í•™ìƒì…ë‹ˆë‹¤. â†’ I am a student**
		{: .notice--primary}
	
		- 1ë²ˆ ì–´í…ì…˜ : `ì €ëŠ” í•™ìƒì…ë‹ˆë‹¤.`ë¼ëŠ” ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ ê°„ì˜ ìœ ì‚¬ë„ ì¸¡ì •
			- ëª©ì  : í•œêµ­ì–´ì˜ ì–´ìˆœ ë°ì´í„°?
		- 2ë²ˆ ì–´í…ì…˜ : `I am a student`ë¼ëŠ” ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ ê°„ì˜ ìœ ì‚¬ë„ ì¸¡ì •
			- ëª©ì  : ì˜ì–´ì˜ ì–´ìˆœ ë°ì´í„°?
		- 3ë²ˆ ì–´í…ì…˜ : `ë‘ ë¬¸ì¥`ì˜ ë‹¨ì–´ë“¤ ê°„ì˜ ìœ ì‚¬ë„ ì¸¡ì •
			- ëª©ì  : í•œêµ­ì–´-ì˜ì–´ê°„ì˜ ë‹¨ì–´ ìœ ì‚¬ë„ ë°ì´í„°?

---

- ì½”ë“œ êµ¬í˜„
```python
# ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜
def scaled_dot_product_attention(query, key, value, mask):
	# ì–´í…ì…˜ ê°€ì¤‘ì¹˜ëŠ” Qì™€ Kì˜ ë‹· í”„ë¡œë•íŠ¸
	matmul_qk = tf.matmul(query, key, transpose_b=True) # í–‰ë ¬ë‚´ì , bì „ì¹˜
	
	# ê°€ì¤‘ì¹˜ë¥¼ ì •ê·œí™”
	depth = tf.cast(tf.shape(key)[-1], tf.float32) # key í–‰ê°œìˆ˜
	logits = matmul_qk / tf.math.sqrt(depth)
	
	# íŒ¨ë”©ì— ë§ˆìŠ¤í¬ ì¶”ê°€(íŠ¹ì • ë‹¨ì–´ ë¬´ì‹œ)
	if mask is not None:
	logits += (mask * -1e9) # ë¬´í•œí•œ ìŒìˆ˜ê°’
	
	# softmaxì ìš©
	attention_weights = tf.nn.softmax(logits, axis=-1)
	
	# ìµœì¢… ì–´í…ì…˜ì€ ê°€ì¤‘ì¹˜ì™€ Vì˜ ë‹· í”„ë¡œë•íŠ¸
	output = tf.matmul(attention_weights, value)
	return output
```

### 3) ë©€í‹° í—¤ë“œ ì–´í…ì…˜
- ì •ì˜ : ë‹¤ìˆ˜ì˜ ì–´í…ì…˜ ë³‘ë ¬ ìˆ˜í–‰<br/>
	![img14](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_18_nnOTx9p.png)
	- í•œë²ˆ ìˆ˜í–‰ ì‹œ ë†“ì¹  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ìºì¹˜

---

- ì½”ë“œ êµ¬í˜„
```python
class MultiHeadAttention(tf.keras.layers.Layer):
  
	def __init__(self, d_model, num_heads, name="multi_head_attention"):
		super(MultiHeadAttention, self).__init__(name=name)
		self.num_heads = num_heads
		self.d_model = d_model
		
		assert d_model % self.num_heads == 0
		
		self.depth = d_model // self.num_heads # ê° ì–´í…ì…˜ë³„ ëìŠ¤
		
		self.query_dense = tf.keras.layers.Dense(units=d_model)
		self.key_dense = tf.keras.layers.Dense(units=d_model)
		self.value_dense = tf.keras.layers.Dense(units=d_model)
		
		self.dense = tf.keras.layers.Dense(units=d_model)

	def split_heads(self, inputs, batch_size):
		inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self.depth))
		return tf.transpose(inputs, perm=[0, 2, 1, 3])

	def call(self, inputs):
		query, key, value, mask = inputs['query'], inputs['key'], inputs[
			'value'], inputs['mask']
		batch_size = tf.shape(query)[0]
		
		# Q, K, Vì— ê°ê° Denseë¥¼ ì ìš©í•©ë‹ˆë‹¤
		query = self.query_dense(query)
		key = self.key_dense(key)
		value = self.value_dense(value)
		
		# ë³‘ë ¬ ì—°ì‚°ì„ ìœ„í•œ ë¨¸ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œ ë§Œë“­ë‹ˆë‹¤
		query = self.split_heads(query, batch_size)
		key = self.split_heads(key, batch_size)
		value = self.split_heads(value, batch_size)
		
		# ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜
		scaled_attention = scaled_dot_product_attention(query, key, value, mask)
		
		scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])
		
		# ì–´í…ì…˜ ì—°ì‚° í›„ì— ê° ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì—°ê²°(concatenate)í•©ë‹ˆë‹¤
		concat_attention = tf.reshape(scaled_attention,
									  (batch_size, -1, self.d_model))
		
		# ìµœì¢… ê²°ê³¼ì—ë„ Denseë¥¼ í•œ ë²ˆ ë” ì ìš©í•©ë‹ˆë‹¤
		outputs = self.dense(concat_attention)
		
		return outputs
	```

### 4) ë§ˆìŠ¤í‚¹
- ì •ì˜ : íŠ¹ì • ê°’ë“¤ì„ ê°€ë ¤ì„œ ì‹¤ì œ ì—°ì‚°ì— ë°©í•´ê°€ ë˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ê¸°ë²•
- ##### íŒ¨ë”© ë§ˆìŠ¤í‚¹(Padding Masking)
	![img15](https://d3s0tskafalll9.cloudfront.net/media/images/1365906-20200410103623697-871078599.max-800x600.png)

	- ì½”ë“œ êµ¬í˜„	
		```python
		def create_padding_mask(x):
			mask = tf.cast(tf.math.equal(x, 0), tf.float32)
			return mask[:, tf.newaxis, tf.newaxis, :] # (batch_size, 1, 1, sequence length)
		```

- ##### ë£© ì–´í—¤ë“œ ë§ˆìŠ¤í‚¹(Look-ahead masking, ë‹¤ìŒ ë‹¨ì–´ ê°€ë¦¬ê¸°)
	- RNNì€ êµ¬ì¡° ìƒ ìì‹ ë³´ë‹¤ ì•ì— ìˆë˜ ë‹¨ì–´ë“¤ë§Œ ì°¸ê³ í•´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡ê°€ëŠ¥
	- ë°˜ë©´ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ìœ„ì¹˜ì™€ ìƒê´€ì—†ì´ ëª¨ë“  ë‹¨ì–´ë¥¼ ì°¸ê³ í•´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡ê°€ëŠ¥
		- ê·¸ëŸ¬ë‚˜ ì´ì „ ë‹¨ì–´ë“¤ë¡œë¶€í„° ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í›ˆë ¨ì´ í•„ìš”
			â†’ **ìì‹ ë³´ë‹¤ ë‹¤ìŒì— ë‚˜ì˜¬ ë‹¨ì–´ë¥¼ ì°¸ê³ í•˜ì§€ ì•Šë„ë¡ ê°€ë¦¬ëŠ” ê¸°ë²•**
	![img16](https://d3s0tskafalll9.cloudfront.net/media/images/_.max-800x600.png)

	- ì½”ë“œ êµ¬í˜„
		```python
		def create_look_ahead_mask(x):
			seq_len = tf.shape(x)[1]
			look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)
			padding_mask = create_padding_mask(x)
			return tf.maximum(look_ahead_mask, padding_mask)
		```

### 5) ì¸ì½”ë”
- 2ê°œì˜ ì„œë¸Œ ì¸µ : ì…€í”„ ì–´í…ì…˜, í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§
![img17](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_21_Y7Cy8sm.max-800x600.png)

---

- ì½”ë“œ êµ¬í˜„1 : í•˜ë‚˜ì˜ ì¸ì½”ë”
```python
# ì¸ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.
# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ë‘ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.
def encoder_layer(units, d_model, num_heads, dropout, name="encoder_layer"):
	inputs = tf.keras.Input(shape=(None, d_model), name="inputs")
	
	# íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©
	padding_mask = tf.keras.Input(shape=(1, 1, None), name="padding_mask")
	
	# ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)
	attention = MultiHeadAttention(
	  d_model, num_heads, name="attention")({
		  'query': inputs,
		  'key': inputs,
		  'value': inputs,
		  'mask': padding_mask
	  })
	
	# ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” Dropoutê³¼ Layer Normalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰
	attention = tf.keras.layers.Dropout(rate=dropout)(attention)
	attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)
	
	# ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ
	outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)
	outputs = tf.keras.layers.Dense(units=d_model)(outputs)
	
	# ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰
	outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
	outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + outputs)
	
	return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)
```

<br/>

- ì½”ë“œ êµ¬í˜„2 : ì¸ì½”ë” ìŒ“ê¸°
```python
def encoder(vocab_size,
Â  Â  Â  Â  Â  Â  num_layers,
Â  Â  Â  Â  Â  Â  units,
Â  Â  Â  Â  Â  Â  d_model,
Â  Â  Â  Â  Â  Â  num_heads,
Â  Â  Â  Â  Â  Â  dropout,
Â  Â  Â  Â  Â  Â  name="encoder"):
	inputs = tf.keras.Input(shape=(None,), name="inputs")
	
	# íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©
	padding_mask = tf.keras.Input(shape=(1, 1, None), name="padding_mask")
	
	# ì„ë² ë”© ë ˆì´ì–´
	embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
	embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))
	
	# í¬ì§€ì…”ë„ ì¸ì½”ë”©
	embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)
	
	outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)
	
	# num_layersë§Œí¼ ìŒ“ì•„ì˜¬ë¦° ì¸ì½”ë”ì˜ ì¸µ.
	for i in range(num_layers):
		outputs = encoder_layer(
			units=units,
			d_model=d_model,
			num_heads=num_heads,
			dropout=dropout,
			name="encoder_layer_{}".format(i),
		)([outputs, padding_mask])
	
	return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)
```

### 6) ë””ì½”ë”
- ì„¸ê°œì˜ ì„œë¸Œì¸µ : ì…€í”„ ì–´í…ì…˜, ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜, í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§
	![img18](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_23_vBHZ3i0.max-800x600.png)
	![img19](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_24_Kj9egLY.max-800x600.png)

---

- ì½”ë“œ êµ¬í˜„1 : í•˜ë‚˜ì˜ ë””ì½”ë”
```python
# ë””ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.
# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ì„¸ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.
def decoder_layer(units, d_model, num_heads, dropout, name="decoder_layer"):
	inputs = tf.keras.Input(shape=(None, d_model), name="inputs")
	enc_outputs = tf.keras.Input(shape=(None, d_model), name="encoder_outputs")
	look_ahead_mask = tf.keras.Input(shape=(1, None, None), name="look_ahead_mask")
	padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')
	
	# ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)
	attention1 = MultiHeadAttention(
	  d_model, num_heads, name="attention_1")(inputs={
		  'query': inputs,
		  'key': inputs,
		  'value': inputs,
		  'mask': look_ahead_mask
	  })
	
	# ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰
	attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)

	# ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜)
	attention2 = MultiHeadAttention(
	  d_model, num_heads, name="attention_2")(inputs={
		  'query': attention1,
		  'key': enc_outputs,
		  'value': enc_outputs,
		  'mask': padding_mask
	  })
	
	# ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ”
	# Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰
	attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)
	attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)
	
	# ì„¸ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ
	outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)	
	outputs = tf.keras.layers.Dense(units=d_model)(outputs)
	
	# ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalization ìˆ˜í–‰
	outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
	outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)

	return tf.keras.Model(
	  inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],
	  outputs=outputs,
	  name=name)
```

<br/>

- ì½”ë“œ êµ¬í˜„2 : ë””ì½”ë” ìŒ“ê¸°
```python
def decoder(vocab_size,
Â  Â  Â  Â  Â  Â  num_layers,
Â  Â  Â  Â  Â  Â  units,
Â  Â  Â  Â  Â  Â  d_model,
Â  Â  Â  Â  Â  Â  num_heads,
Â  Â  Â  Â  Â  Â  dropout,
Â  Â  Â  Â  Â  Â  name='decoder'):
	inputs = tf.keras.Input(shape=(None,), name='inputs')
	enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')
	look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')
	
	# íŒ¨ë”© ë§ˆìŠ¤í¬
	padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')
	
	# ì„ë² ë”© ë ˆì´ì–´
	embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
	embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))
	
	# í¬ì§€ì…”ë„ ì¸ì½”ë”©
	embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)
	
	# Dropoutì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰
	outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)
	
	for i in range(num_layers):
	Â  Â  outputs = decoder_layer(
	Â  Â  Â  Â  units=units,
	Â  Â  Â  Â  d_model=d_model,
	Â  Â  Â  Â  num_heads=num_heads,
	Â  Â  Â  Â  dropout=dropout,
	Â  Â  Â  Â  name='decoder_layer_{}'.format(i),
Â  Â  )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])

	return tf.keras.Model(
Â  Â  Â  inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],
Â  Â  Â  outputs=outputs,
Â  Â  Â  name=name)
```


### 7) ì±—ë´‡ ë°ì´í„° ë¡œë“œ
- Cornell Movie-Dialogs Corpus ì˜í™” ë° TV í”„ë¡œê·¸ë¨ì—ì„œ ì‚¬ìš©ë˜ì—ˆë˜ ëŒ€í™”ì˜ ìŒìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ì„ ì‚¬ìš©
- ëª©í‘œ
	1. ì •í•´ì§„Â ê°œìˆ˜ì¸Â 50,000ê°œì˜Â ì§ˆë¬¸ê³¼Â ë‹µë³€ì˜Â ìŒì„Â ì¶”ì¶œí•œë‹¤.
	2. ë¬¸ì¥ì—ì„œ ë‹¨ì–´ì™€ êµ¬ë‘ì  ì‚¬ì´ì— ê³µë°±ì„ ì¶”ê°€í•œë‹¤.
	3. ì•ŒíŒŒë²³ê³¼ ! ? , . ì´ 4ê°œì˜ êµ¬ë‘ì ì„ ì œì™¸í•˜ê³  ë‹¤ë¥¸ íŠ¹ìˆ˜ë¬¸ìëŠ” ëª¨ë‘ ì œê±°í•œë‹¤.

---

- ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
	```python
	path_to_zip = tf.keras.utils.get_file(
	Â  Â  'cornell_movie_dialogs.zip',
	Â  Â  origin='http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip', extract=True)

	path_to_dataset = os.path.join(os.path.dirname(path_to_zip), "cornell movie-dialogs corpus") 

	path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')
	path_to_movie_conversations = os.path.join(path_to_dataset,'movie_conversations.txt')
	```

<br/>

- ìƒ˜í”Œ ìµœëŒ€ ê°œìˆ˜ ì§€ì •
	```python
	# ì‚¬ìš©í•  ìƒ˜í”Œì˜ ìµœëŒ€ ê°œìˆ˜
	MAX_SAMPLES = 50000
	```

<br/>

- ìì—°ì–´ ì „ì²˜ë¦¬
	```python
	# ì „ì²˜ë¦¬ í•¨ìˆ˜
	def preprocess_sentence(sentence):
		# ì…ë ¥ë°›ì€ sentenceë¥¼ ì†Œë¬¸ìë¡œ ë³€ê²½í•˜ê³  ì–‘ìª½ ê³µë°±ì„ ì œê±°
		sentence = sentence.lower().strip()
		
		# ë‹¨ì–´ì™€ êµ¬ë‘ì (punctuation) ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.
		# ì˜ˆë¥¼ ë“¤ì–´ì„œ "I am a student." => "I am a student ."ì™€ ê°™ì´
		# studentì™€ ì˜¨ì  ì‚¬ì´ì— ê±°ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.
		sentence = re.sub(r"([?.!,])", r" \1 ", sentence)
		sentence = re.sub(r'[" "]+', " ", sentence)
		
		# (a-z, A-Z, ".", "?", "!", ",")ë¥¼ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ê³µë°±ì¸ ' 'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤
		sentence = re.sub(r"[^a-zA-Z?.!,]+", " ", sentence)
		sentence = sentence.strip()
		return sentence
	```

<br/>

- Paired data ë§Œë“¤ê¸°
	```python
	# ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ìŒì¸ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ê¸° ìœ„í•œ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
	def load_conversations():
		id2line = {}
		with open(path_to_movie_lines, errors='ignore') as file:
			lines = file.readlines()
		for line in lines:
			parts = line.replace('\n', '').split(' +++$+++ ')
			id2line[parts[0]] = parts[4]
		
		inputs, outputs = [], []
		with open(path_to_movie_conversations, 'r') as file:
			lines = file.readlines()
		
		for line in lines:
			parts = line.replace('\n', '').split(' +++$+++ ')
			conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]
		
			for i in range(len(conversation) - 1):
			# ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì§ˆë¬¸ì— í•´ë‹¹ë˜ëŠ” inputsì™€ ë‹µë³€ì— í•´ë‹¹ë˜ëŠ” outputsì— ì ìš©.
				inputs.append(preprocess_sentence(id2line[conversation[i]]))
				outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))
		
			if len(inputs) >= MAX_SAMPLES:
				return inputs, outputs
		return inputs, outputs
	```

<br/>

- ìƒ˜í”Œ í™•ì¸
	```python
	# ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ì—¬ ì§ˆë¬¸ì„ questions, ë‹µë³€ì„ answersì— ì €ì¥í•©ë‹ˆë‹¤.
	questions, answers = load_conversations()
	print('ì „ì²´ ìƒ˜í”Œ ìˆ˜ :', len(questions))
	print('ì „ì²´ ìƒ˜í”Œ ìˆ˜ :', len(answers))

	print('ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(questions[21]))
	print('ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(answers[21]))
	```

### 8) ë³‘ë ¬ ë°ì´í„° ì „ì²˜ë¦¬
- ë‹¨ì–´ì¥ ë§Œë“¤ê¸°
	```python
	import tensorflow_datasets as tfds

	# ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ Vocabulary ìƒì„±
	tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)

	# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ê³ ìœ í•œ ì •ìˆ˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
	START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]
	```

<br/>

- ì‹œì‘, ì¢…ë£Œ í† í° ë²ˆí˜¸ í™•ì¸, ì‚¬ì´ì¦ˆ ì¡°ì •
	```python
	print('START_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size])
	print('END_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size + 1])

	# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ê³ ë ¤í•˜ì—¬ +2ë¥¼ í•˜ì—¬ ë‹¨ì–´ì¥ì˜ í¬ê¸°ë¥¼ ì‚°ì •í•©ë‹ˆë‹¤.
	VOCAB_SIZE = tokenizer.vocab_size + 2
	```

<br/>

- ì •ìˆ˜ ì¸ì½”ë”©(Integer encoding) & íŒ¨ë”©(Padding) ë³€í™˜ê²°ê³¼ í™•ì¸
	```python
	# ì„ì˜ì˜ 22ë²ˆì§¸ ìƒ˜í”Œì— ëŒ€í•´ì„œ ì •ìˆ˜ ì¸ì½”ë”© ì‘ì—…ì„ ìˆ˜í–‰.
	# ê° í† í°ì„ ê³ ìœ í•œ ì •ìˆ˜ë¡œ ë³€í™˜
	print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(tokenizer.encode(questions[21])))
	print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(tokenizer.encode(answers[21])))
	```

<br/>

- ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ ì§€ì •(íŒ¨ë”©(Padding) ê°’)
	```python
	# ìƒ˜í”Œì˜ ìµœëŒ€ í—ˆìš© ê¸¸ì´ ë˜ëŠ” íŒ¨ë”© í›„ì˜ ìµœì¢… ê¸¸ì´
	MAX_LENGTH = 40

	def tokenize_and_filter(inputs, outputs):
		tokenized_inputs, tokenized_outputs = [], []

		for (sentence1, sentence2) in zip(inputs, outputs):
		Â  Â  # ì •ìˆ˜ ì¸ì½”ë”© ê³¼ì •ì—ì„œ ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì¶”ê°€
		Â  Â  sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN
		Â  Â  sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN

	Â  Â  # ìµœëŒ€ ê¸¸ì´ 40 ì´í•˜ì¸ ê²½ìš°ì—ë§Œ ë°ì´í„°ì…‹ìœ¼ë¡œ í—ˆìš©
		if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:
			tokenized_inputs.append(sentence1)
			tokenized_outputs.append(sentence2)
		
		# ìµœëŒ€ ê¸¸ì´ 40ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ì…‹ì„ íŒ¨ë”©
		tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs, maxlen=MAX_LENGTH, padding='post')
		
		tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs, maxlen=MAX_LENGTH, padding='post')
		
		return tokenized_inputs, tokenized_outputs
	```

<br/>

- ìƒ˜í”Œ ê¸¸ì´ 40ì„ ë„˜ì€ ê²½ìš° ì¼ë¶€ ìƒ˜í”Œ ì œì™¸
	```python
	questions, answers = tokenize_and_filter(questions, answers)
	print('ë‹¨ì–´ì¥ì˜ í¬ê¸° :',(VOCAB_SIZE))
	print('í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(questions)))
	print('í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(answers)))
	```

<br/>

- êµì‚¬ ê°•ìš”(Teacher Forcing) ì‚¬ìš©
	- ê°œë… : `tì‹œì  ì˜ˆì¸¡ ê°’`ì„ `t+1 ì…ë ¥`ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , `t+1ì‹œì  ì •ë‹µ`ì„ ì‚¬ìš©
	- ëª©ì  : í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ ê°œì„ (ë¯¸ì‚¬ìš© ì‹œ, ì˜ëª»ëœ ì˜ˆì¸¡ í•˜ë‚˜ê°€ ì—°ì‡„ì ìœ¼ë¡œ ë‹¤ìŒ ì˜ˆì¸¡ ì •í™•ë„ì— ì˜í–¥ì„ ë¯¸ì¹¨ /  start_token ì œê±°)

	```python
	BATCH_SIZE = 64
	BUFFER_SIZE = 20000

	# ë””ì½”ë”ëŠ” ì´ì „ì˜ targetì„ ë‹¤ìŒì˜ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
	# ì´ì— ë”°ë¼ outputsì—ì„œëŠ” START_TOKENì„ ì œê±°í•˜ê² ìŠµë‹ˆë‹¤.
	dataset = tf.data.Dataset.from_tensor_slices((
	Â  Â  {
	Â  Â  Â  Â  'inputs': questions,
	Â  Â  Â  Â  'dec_inputs': answers[:, :-1] # end_token ì œê±°
	Â  Â  },
	Â  Â  {
	Â  Â  Â  Â  'outputs': answers[:, 1:] # start_token ì œê±°
	Â  Â  },
	))

	dataset = dataset.cache()
	dataset = dataset.shuffle(BUFFER_SIZE)
	dataset = dataset.batch(BATCH_SIZE)
	dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
	```

### 9) ëª¨ë¸ ì •ì˜ ë° í•™ìŠµí•˜ê¸°
- íŠ¸ëœìŠ¤í¬ë¨¸ í•¨ìˆ˜ ì •ì˜
	```python
	def transformer(vocab_size,
	Â  Â  Â  Â  Â  Â  Â  Â  num_layers,
	Â  Â  Â  Â  Â  Â  Â  Â  units,
	Â  Â  Â  Â  Â  Â  Â  Â  d_model,
	Â  Â  Â  Â  Â  Â  Â  Â  num_heads,
	Â  Â  Â  Â  Â  Â  Â  Â  dropout,
	Â  Â  Â  Â  Â  Â  Â  Â  name="transformer"):
		inputs = tf.keras.Input(shape=(None,), name="inputs")
		dec_inputs = tf.keras.Input(shape=(None,), name="dec_inputs")
		
		# ì¸ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬
		enc_padding_mask = tf.keras.layers.Lambda(
		create_padding_mask, output_shape=(1, 1, None),
		name='enc_padding_mask')(inputs)
		
		# ë””ì½”ë”ì—ì„œ ë¯¸ë˜ì˜ í† í°ì„ ë§ˆìŠ¤í¬ í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
		# ë‚´ë¶€ì ìœ¼ë¡œ íŒ¨ë”© ë§ˆìŠ¤í¬ë„ í¬í•¨ë˜ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
		look_ahead_mask = tf.keras.layers.Lambda(
		create_look_ahead_mask,
		output_shape=(1, None, None),
		name='look_ahead_mask')(dec_inputs)
		
		# ë‘ ë²ˆì§¸ ì–´í…ì…˜ ë¸”ë¡ì—ì„œ ì¸ì½”ë”ì˜ ë²¡í„°ë“¤ì„ ë§ˆìŠ¤í‚¹
		# ë””ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬
		dec_padding_mask = tf.keras.layers.Lambda(
		create_padding_mask, output_shape=(1, 1, None),
		name='dec_padding_mask')(inputs)
		
		# ì¸ì½”ë”
		enc_outputs = encoder(
		vocab_size=vocab_size,
		num_layers=num_layers,
		units=units,
		d_model=d_model,
		num_heads=num_heads,
		dropout=dropout,
		)(inputs=[inputs, enc_padding_mask])
		
		# ë””ì½”ë”
		dec_outputs = decoder(
		vocab_size=vocab_size,
		num_layers=num_layers,
		units=units,
		d_model=d_model,
		num_heads=num_heads,
		dropout=dropout,
		)(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])
		
		# ì™„ì „ì—°ê²°ì¸µ
		outputs = tf.keras.layers.Dense(units=vocab_size, name="outputs")(dec_outputs)
		
		return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)
	```

<br/>

- ëª¨ë¸ ìƒì„±
	```python
	tf.keras.backend.clear_session()

	# í•˜ì´í¼íŒŒë¼ë¯¸í„°
	NUM_LAYERS = 2 # ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì¸µì˜ ê°œìˆ˜
	D_MODEL = 256 # ì¸ì½”ë”ì™€ ë””ì½”ë” ë‚´ë¶€ì˜ ì…, ì¶œë ¥ì˜ ê³ ì • ì°¨ì›
	NUM_HEADS = 8 # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ í—¤ë“œ ìˆ˜
	UNITS = 512 # í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°
	DROPOUT = 0.1 # ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨

	model = transformer(
	Â  Â  vocab_size=VOCAB_SIZE,
	Â  Â  num_layers=NUM_LAYERS,
	Â  Â  units=UNITS,
	Â  Â  d_model=D_MODEL,
	Â  Â  num_heads=NUM_HEADS,
	Â  Â  dropout=DROPOUT)

	model.summary()
	```

<br/>

- ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
	```python
	def loss_function(y_true, y_pred):
		y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))
		loss = tf.keras.losses.SparseCategoricalCrossentropy(
		from_logits=True, reduction='none')(y_true, y_pred)
		
		mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)
		loss = tf.multiply(loss, mask)
		
		return tf.reduce_mean(loss)
	```

<br/>

- ì»¤ìŠ¤í…€ ëœ í•™ìŠµ : í•™ìŠµë¥ ì„ ì´ˆê¸°ì— ê¸‰ê²©íˆ ë†’ì˜€ë‹¤ê°€, ì„œì„œíˆ ë‚®ì¶”ëŠ” ë°©ì‹
	- ìˆ˜ì‹ : $lrate = d^{-0.5}_{model}Â·min(step_num^{-0.5}, step_numÂ·warmup_step^{-.15})$

	```python
	class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):

		def __init__(self, d_model, warmup_steps=4000):
			super(CustomSchedule, self).__init__()
		
			self.d_model = d_model
			self.d_model = tf.cast(self.d_model, tf.float32)
		
			self.warmup_steps = warmup_steps
		
		def __call__(self, step):
			arg1 = tf.math.rsqrt(step)
			arg2 = step * (self.warmup_steps**-1.5)
		
			return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)
	```
	
<br/>

- í•™ìŠµë¥  ë³€í™” ì‹œê°í™”
	```python
	sample_learning_rate = CustomSchedule(d_model=128)

	plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))
	plt.ylabel("Learning Rate")
	plt.xlabel("Train Step")
	```
	
<br/>

- ëª¨ë¸ ì»´íŒŒì¼
	```python
	learning_rate = CustomSchedule(D_MODEL)

	optimizer = tf.keras.optimizers.Adam(
	Â  Â  learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)

	def accuracy(y_true, y_pred):
		y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))
		return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)

	model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])
	```

<br/>

- í•™ìŠµ ì‹œí‚¤ê¸°
	```python
	EPOCHS = 10
	model.fit(dataset, epochs=EPOCHS, verbose=1)
	```

### 10) ì±—ë´‡ í…ŒìŠ¤íŠ¸í•˜ê¸°
- ì˜ˆì¸¡ ê³¼ì •
	1. ìƒˆë¡œìš´Â ì…ë ¥Â ë¬¸ì¥ì—Â ëŒ€í•´ì„œëŠ”Â í›ˆë ¨ ë•Œì™€Â ë™ì¼í•œÂ ì „ì²˜ë¦¬ë¥¼Â ê±°ì¹œë‹¤.
	2. ì…ë ¥Â ë¬¸ì¥ì„Â í† í¬ë‚˜ì´ì§•í•˜ê³ ,Â `START_TOKEN`ê³¼Â `END_TOKEN`ì„Â ì¶”ê°€í•œë‹¤.
	3. íŒ¨ë”©Â ë§ˆìŠ¤í‚¹ê³¼Â ë£©Â ì–´í—¤ë“œÂ ë§ˆìŠ¤í‚¹ì„Â ê³„ì‚°í•œë‹¤.
	4. ë””ì½”ë”ëŠ”Â ì…ë ¥Â ì‹œí€€ìŠ¤ë¡œë¶€í„°Â ë‹¤ìŒÂ ë‹¨ì–´ë¥¼Â ì˜ˆì¸¡í•œë‹¤.
	5. ë””ì½”ë”ëŠ”Â ì˜ˆì¸¡ëœÂ ë‹¤ìŒÂ ë‹¨ì–´ë¥¼Â ê¸°ì¡´ì˜Â ì…ë ¥Â ì‹œí€€ìŠ¤ì—Â ì¶”ê°€í•˜ì—¬Â ìƒˆë¡œìš´Â ì…ë ¥ìœ¼ë¡œÂ ì‚¬ìš©í•œë‹¤.
	6. `END_TOKEN`ì´Â ì˜ˆì¸¡ë˜ê±°ë‚˜Â ë¬¸ì¥ì˜Â ìµœëŒ€Â ê¸¸ì´ì—Â ë„ë‹¬í•˜ë©´Â ë””ì½”ë”ëŠ”Â ë™ì‘ì„Â ë©ˆì¶˜ë‹¤.

---

- ì˜ˆì¸¡ ê³¼ì • êµ¬í˜„
	```python
	def decoder_inference(sentence):
		sentence = preprocess_sentence(sentence)

		# ì…ë ¥ëœ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”© í›„, ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì•ë’¤ë¡œ ì¶”ê°€.
		# ex) Where have you been? â†’ [[8331 Â  86 Â  30 Â  Â 5 1059 Â  Â 7 8332]]
		sentence = tf.expand_dims(
	Â  Â  Â  START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)
	
		# ë””ì½”ë”ì˜ í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ë³€ìˆ˜.
		# ì²˜ìŒì—ëŠ” ì˜ˆì¸¡í•œ ë‚´ìš©ì´ ì—†ìŒìœ¼ë¡œ ì‹œì‘ í† í°ë§Œ ë³„ë„ ì €ì¥. ex) 8331
		output_sequence = tf.expand_dims(START_TOKEN, 0)
	
		# ë””ì½”ë”ì˜ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„
		for i in range(MAX_LENGTH):
		Â  Â  # ë””ì½”ë”ëŠ” ìµœëŒ€ MAX_LENGTHì˜ ê¸¸ì´ë§Œí¼ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ë°˜ë³µí•©ë‹ˆë‹¤.
		Â  Â  predictions = model(inputs=[sentence, output_sequence], training=False)
		Â  Â  predictions = predictions[:, -1:, :]

		Â  Â  # í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ì˜ ì •ìˆ˜
		Â  Â  predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)

		Â  Â  # ë§Œì•½ í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ forë¬¸ì„ ì¢…ë£Œ
			if tf.equal(predicted_id, END_TOKEN[0]):
				break
		
			# ì˜ˆì¸¡í•œ ë‹¨ì–´ë“¤ì€ ì§€ì†ì ìœ¼ë¡œ output_sequenceì— ì¶”ê°€ë©ë‹ˆë‹¤.
			# ì´ output_sequenceëŠ” ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.
			output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)

		return tf.squeeze(output_sequence, axis=0)
	```
<br/>

- ìƒì„± í•¨ìˆ˜ êµ¬í˜„
	```python
	def sentence_generation(sentence):
		# ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ ë””ì½”ë”ë¥¼ ë™ì‘ ì‹œì¼œ ì˜ˆì¸¡ëœ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŠµë‹ˆë‹¤.
		prediction = decoder_inference(sentence)

		Â  # ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
		predicted_sentence = tokenizer.decode(
	Â  Â  Â  [i for i in prediction if i < tokenizer.vocab_size])

		print('ì…ë ¥ : {}'.format(sentence))
		print('ì¶œë ¥ : {}'.format(predicted_sentence))
		
		return predicted_sentence
	```

<br/>

- ëŒ€ë‹µ í™•ì¸
	```python
	sentence_generation('Where have you been?')
	```

