---
layer: single
title: "Aiffel) Exploration - ìƒì„±í˜• AI(Generative Modeling) ì •ë¦¬"
categories: Exploration
tag: [Generative, ìƒì„±í˜•ai]
---

ë‚´ìš© ì¶œì²˜ : [Aiffel](https://www.aiffel.io/)

### 1. ìƒì„± ëª¨ë¸ë§
#### 1) íŒë³„ ëª¨ë¸(ë¶„ë¥˜, íšŒê·€ ë“±)ê³¼ì˜ ì°¨ì´
- íŒë³„ ëª¨ë¸ : ë¶„ë¥˜, íšŒê·€
- **ìƒì„± ëª¨ë¸** : í•™ìŠµ ë°ì´í„°ì…‹ì— ê¸°ë°˜í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ìƒì„±
	- âœ¨ìµœê·¼ ì˜ˆì‹œ : [AWSì˜ DeepComposer](https://www.youtube.com/watch?v=XH2EbK9dQlg)
		- GAN(ìƒì„±ì  ì ëŒ€ ì‹ ê²½ë§) = Generator(ìƒì„±ì) vs Discriminator(íŒë³„ì) ìœ¼ë¡œ í•™ìŠµì§„í–‰
	- Pix2Pix(ê·¸ë¦¼â†’ì‚¬ì§„ ë³€ê²½ AI)
		!['01'](https://d3s0tskafalll9.cloudfront.net/media/images/pix2pix.max-800x600.png)
		- `ğŸ“Œ ì£¼ìš” ìš©ì–´ : Ground Truth(ì‹¤ì œ ì´ë¯¸ì§€)`
		- âœ¨ ë…¼ë¬¸ì„ í†µí•œ ì´í•´ : [https://arxiv.org/pdf/1611.07004.pdf](https://arxiv.org/pdf/1611.07004.pdf)
		
### 2) ëª¨ë¸1_CycleGAN
- í•œ ì´ë¯¸ì§€ì™€ ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ ë²ˆê°ˆì•„ ê°€ë©° Cyclicí•˜ê²Œ ë³€í™˜
	- paired dataê°€ í•„ìš”X â†’ annotation ë¹„ìš© X
	- ğŸ“Œ `ì£¼ìš” ìš©ì–´ : paired data(ìŒìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„° ì…‹), annotation(ë¼ë²¨ë§)`
		!['02'](https://d3s0tskafalll9.cloudfront.net/media/images/CycleGAN.max-800x600.png)<br/>
<br/>
		!['03'](https://d3s0tskafalll9.cloudfront.net/media/images/CycleGAN2.max-800x600.jpg)

### 3) ëª¨ë¸2_Neural Style Transfer
- ìŠ¤íƒ€ì¼ ë³€í™˜

	!['04'](https://d3s0tskafalll9.cloudfront.net/media/images/StyleTransfer.max-800x600.png)
	- âœ¨í•œ ê°œì˜ Base Imageë¡œ 5ê°€ì§€ ìŠ¤íƒ€ì¼ ë³€í™˜ì„ ì ìš©ì‹œí‚¨ ëª¨ìŠµ
	- Base Imageì˜ Content + Style Imageì˜ Styleì„ í•©
	- `ğŸ“Œ ì£¼ìš” ìš©ì–´ : Content(ë‚´ìš©), Style(ìŠ¤íƒ€ì¼)`


### 4) ì½”ë“œ êµ¬í˜„ì„ í†µí•œ GANì´í•´í•˜ê¸°
!['05'](https://d3s0tskafalll9.cloudfront.net/media/images/GAN.max-800x600.png)

- `ğŸ“Œ Conv2DTranspose(ìƒì„±) â†” Conv2D(íŒë³„)`

#### âœ¨ ì‚¬ì „ ì¤€ë¹„
```python
import os
import glob
import time

import PIL
import imageio
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
from IPython import display
import matplotlib.pyplot as plt
%matplotlib inline

print("tensorflow", tf.__version__)
```

```python
fashion_mnist = tf.keras.datasets.fashion_mnist
(train_x, _), (test_x, _) = fashion_mnist.load_data()

train_x = (train_x - 127.5)/127.5
train_x = train_x.reshape(train_x.shape[0], 28, 28, 1).astype('float32')
plt.imshow(train_x[0].reshape(28, 28), cmap='gray')
plt.colorbar()
plt.show()
```

```python
BUFFER_SIZE = 60000
BATCH_SIZE = 256
train_dataset = tf.data.Dataset.from_tensor_slices(train_x).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
```

#### (1) Generator
- ì½”ë“œ êµ¬í˜„
	```python
	def make_generator_model():
		model = tf.keras.Sequential()
	
		# First: Dense layer
		# unitsì€ 7x7x256, í¸í–¥ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ, ì…ë ¥ ê°’ì˜ í¬ê¸°ëŠ” 100ì°¨ì› ë²¡í„°
		model.add(tf.keras.layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
		model.add(tf.keras.layers.BatchNormalization()) # ë°°ì¹˜ ì •ê·œí™” ì‚¬ìš©
		model.add(tf.keras.layers.LeakyReLU()) # LeakyReLU í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©
	
		# Second: Reshape layer
		model.add(layers.Reshape((7, 7, 256)))
	
		# Third: Conv2DTranspose layer
		# ì»¤ë„ í¬ê¸°ëŠ” 5, strideëŠ” 1, íŒ¨ë”©ì€ ì‚¬ìš©, í¸í–¥ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
		model.add(tf.keras.layers.Conv2DTranspose(128, kernel_size=5, use_bias=False, strides=1, padding='same'))
		model.add(tf.keras.layers.BatchNormalization()) # ë°°ì¹˜ ì •ê·œí™” ì‚¬ìš©
		model.add(tf.keras.layers.LeakyReLU()) # LeakyReLU í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©
	
		# Fourth: Conv2DTranspose layer
		# ì»¤ë„ í¬ê¸°ëŠ” 5, strideëŠ” 2, íŒ¨ë”©ì€ ì‚¬ìš©, í¸í–¥ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
		model.add(tf.keras.layers.Conv2DTranspose(128, kernel_size=5, use_bias=False, strides=2, padding='same'))
		model.add(tf.keras.layers.BatchNormalization()) # ë°°ì¹˜ ì •ê·œí™” ì‚¬ìš©
		model.add(tf.keras.layers.LeakyReLU()) # LeakyReLU í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©
	
		# Fifth: Conv2DTranspose layer
		model.add(tf.keras.layers.Conv2DTranspose(1, kernel_size=(5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
	
		return model
	```

	```python
	generator = make_generator_model()
	generator.summary()
	```
<br/>

- í™•ì¸
	```python
	noise = tf.random.normal([1, 100])
	generated_image = generator(noise, training=False)
	plt.imshow(generated_image[0, :, :, 0], cmap='gray')
	plt.colorbar()
	plt.show()
	```

#### (2) discriminator
- ì •ì˜
	```python
	def make_discriminator_model():
		model = tf.keras.Sequential()
	
		# First: Conv2D Layer
		# í•„í„°ì˜ í¬ê¸°ëŠ” 5ì´ë©° 64ê°œ ì‚¬ìš©, strideëŠ” 2, íŒ¨ë”©ì€ ì‚¬ìš©, ì…ë ¥ ê°’ì˜ í¬ê¸°ëŠ” ì¤€ë¹„ëœ í‘ë°± ì‚¬ì§„
		model.add(tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding='same', input_shape=(28, 28, 1)))
		model.add(tf.keras.layers.LeakyReLU()) # LeakyReLU í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©
		model.add(tf.keras.layers.Dropout(0.3)) # Dropoutì€ 0.3ì„ ì‚¬ìš©
	
		# Second: Conv2D Layer
		# í•„í„°ì˜ í¬ê¸°ëŠ” 5ì´ë©° 128ê°œ ì‚¬ìš©, strideëŠ” 2, íŒ¨ë”©ì€ ì‚¬ìš©
		model.add(tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding='same'))
		model.add(tf.keras.layers.LeakyReLU()) # LeakyReLU í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©
		model.add(tf.keras.layers.Dropout(0.3)) # Dropoutì€ 0.3ì„ ì‚¬ìš©
	  
		# Third: Flatten Layer
		model.add(layers.Flatten())
	  
		# Fourth: Dense Layer
		model.add(layers.Dense(1))
	
		return model
	```

	```python
	discriminator = make_discriminator_model()
	discriminator.summary()
	```

<br/>

- í™•ì¸
	```python
	decision = discriminator(generated_image, training=False)
	decision
	```

#### (3) Loss function
- Cross Entropy : ë‘ ê°’ì´ ì–¼ë§ˆë‚˜ í° ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ê³„ì‚°
	- Real Image ë¼ë²¨ : 1 / Fake Image ë¼ë²¨ : 0ë¡œ ê°€ì • ì‹œ
		- ìƒì„±ìì˜ í•™ìŠµ ëª©í‘œ : Fake Imageì— ëŒ€í•´ íŒë³„ê°’ì´ 1ì— ê°€ê¹Œì›Œì§€ëŠ” ê²ƒ
		- íŒë³„ìì˜ í•™ìŠµ ëª©í‘œ : Real Image íŒë³„ê°’ì´ 1, Fake Image íŒë³„ê°’ì´ 0ì— ê°€ê¹Œì›Œì§€ëŠ” ê²ƒ
- ì½”ë“œ êµ¬í˜„
	```python
	cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)
	```

	- generator_lossëŠ”Â `fake_outputê°€ 1ì— ê°€ê¹Œì›Œì§€ëŠ” ê²ƒ`ì´ ëª©í‘œ
	```python
	def generator_loss(fake_output):
		return cross_entropy(tf.ones_like(fake_output), fake_output)
	```

	- discriminator_lossëŠ”Â `real_outputì€ 1,Â fake_outputì€ 0ì— ê°€ê¹Œì›Œì§€ëŠ” ê²ƒ`ì´ ëª©í‘œ
	```python
	def discriminator_loss(real_output, fake_output):
		# ì†ì‹¤í•¨ìˆ˜ëŠ” cross entropyë¥¼ ì‚¬ìš©, ones_likeë¥¼ í™œìš©
		real_loss = cross_entropy(tf.ones_like(real_output), real_output)
		# ì†ì‹¤í•¨ìˆ˜ëŠ” cross entropyë¥¼ ì‚¬ìš©, zeros_likeë¥¼ í™œìš©
		fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
		total_loss = real_loss + fake_loss # real_lossì™€ fake_lossë¥¼ ë”í•œ ê°’
		return total_loss
	```

#### (4) Accuracy
- ëª©í‘œ : 1ì´ ì•„ë‹Œ `ë‘˜ë‹¤ 0.5ì— ê°€ê¹Œì›Œì§€ëŠ” ê²ƒ`(1ì´ë©´ íŒë³„ìê°€ ë§¤ìš° ì‰½ê²Œ íŒë³„í•´ ë‚´ê³  ìˆë‹¤ëŠ” ëœ»)
- ì½”ë“œ êµ¬í˜„
	```python
	def discriminator_accuracy(real_output, fake_output):
		real_accuracy = tf.reduce_mean(tf.cast(tf.math.greater_equal(real_output, tf.constant([0.5])), tf.float32))
		fake_accuracy = tf.reduce_mean(tf.cast(tf.math.less(fake_output, tf.constant([0.5])), tf.float32))
		return real_accuracy, fake_accuracy
	```

#### (5) Optimizer : Adam
- ì½”ë“œ êµ¬í˜„(`learning rateëŠ” 0.0001`)
	```python
	generator_optimizer = tf.keras.optimizers.Adam(1e-4)
	discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)
	```

#### (6) í•™ìŠµ ì„¤ê³„
- ìƒ˜í”Œ ìƒì„±
```python
noise_dim = 100
num_examples_to_generate = 16
seed = tf.random.normal([num_examples_to_generate, noise_dim])
seed.shape
```

<br/>

- í›ˆë ¨ ê³¼ì • ì„¤ê³„(tf.function ë°ì½” ë ˆì´í„° í™œìš©) : 1 step
	```python
	@tf.function
	def train_step(images): Â #(1) ì…ë ¥ë°ì´í„°
	Â  Â  noise = tf.random.normal([BATCH_SIZE, noise_dim]) Â #(2) ìƒì„±ì ì…ë ¥ ë…¸ì´ì¦ˆ
	Â  Â  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: Â #(3) tf.GradientTape() ì˜¤í”ˆ
	Â  Â  Â  Â  generated_images = generator(noise, training=True) Â #(4) generated_images ìƒì„±

	Â  Â  Â  Â  #(5) discriminator íŒë³„
	Â  Â  Â  Â  real_output = discriminator(images, training=True)
	Â  Â  Â  Â  fake_output = discriminator(generated_images, training=True)

	Â  Â  Â  Â  #(6) loss ê³„ì‚°
	Â  Â  Â  Â  gen_loss = generator_loss(fake_output)
	Â  Â  Â  Â  disc_loss = discriminator_loss(real_output, fake_output)

	Â  Â  Â  Â  #(7) accuracy ê³„ì‚°
	Â  Â  Â  Â  real_accuracy, fake_accuracy = discriminator_accuracy(real_output, fake_output)

	Â  Â  #(8) gradient ê³„ì‚°
	Â  Â  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
	Â  Â  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

	Â  Â  #(9) ëª¨ë¸ í•™ìŠµ
	Â  Â  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
	Â  Â  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))
	
	Â  Â  return gen_loss, disc_loss, real_accuracy, fake_accuracy Â #(10) ë¦¬í„´ê°’
	```

<br/>

- í•™ìŠµ í˜„í™© í™•ì¸
	- ìƒ˜í”Œ ìƒì„± í›„ í™•ì¸
		```python
		def generate_and_save_images(model, epoch, it, sample_seeds):
		Â  Â  predictions = model(sample_seeds, training=False)
		Â  Â  fig = plt.figure(figsize=(4, 4))
		
		Â  Â  for i in range(predictions.shape[0]):
		Â  Â  Â  Â  plt.subplot(4, 4, i+1)
		Â  Â  Â  Â  plt.imshow(predictions[i, :, :, 0], cmap='gray')
		Â  Â  Â  Â  plt.axis('off')
			plt.savefig('sample_epoch_{:04d}_iter_{:03d}.png'.format(epoch, it))
		Â  Â  plt.show()
		```

	- lossì™€ accuracy ê·¸ë˜í”„ ì‹œê°í™”
		```python
		from matplotlib.pylab import rcParams
		rcParams['figure.figsize'] = 15, 6 Â  Â # matlab ì°¨íŠ¸ì˜ ê¸°ë³¸ í¬ê¸°ë¥¼ 15,6ìœ¼ë¡œ ì§€ì •í•´ ì¤ë‹ˆë‹¤.
		
		def draw_train_history(history, epoch):
		Â  Â  # summarize history for loss Â 
		Â  Â  plt.subplot(211) Â 
		Â  Â  plt.plot(history['gen_loss']) Â 
		Â  Â  plt.plot(history['disc_loss']) Â 
		Â  Â  plt.title('model loss') Â 
		Â  Â  plt.ylabel('loss') Â 
		Â  Â  plt.xlabel('batch iters') Â 
		Â  Â  plt.legend(['gen_loss', 'disc_loss'], loc='upper left') Â 
		
		Â  Â  # summarize history for accuracy Â 
		Â  Â  plt.subplot(212) Â 
		Â  Â  plt.plot(history['fake_accuracy']) Â 
		Â  Â  plt.plot(history['real_accuracy']) Â 
		Â  Â  plt.title('discriminator accuracy') Â 
		Â  Â  plt.ylabel('accuracy') Â 
		Â  Â  plt.xlabel('batch iters') Â 
		Â  Â  plt.legend(['fake_accuracy', 'real_accuracy'], loc='upper left') Â 
		
		Â  Â  # training_history ë””ë ‰í† ë¦¬ì— epochë³„ë¡œ ê·¸ë˜í”„ë¥¼ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
			plt.savefig('train_history_{:04d}.png'.format(epoch))
		Â  Â  plt.show()
		```

	- ì²´í¬í¬ì¸íŠ¸ ì •ì˜
		```python
		checkpoint_dir = 'training_checkpoints'
		checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
		checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
		Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â discriminator_optimizer=discriminator_optimizer,
		Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â generator=generator,
		Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â discriminator=discriminator)
		```

#### (7) í•™ìŠµ
- í•™ìŠµ êµ¬í˜„
	```python
	def train(dataset, epochs, save_every):
	Â  Â  start = time.time()
	Â  Â  history = {'gen_loss':[], 'disc_loss':[], 'real_accuracy':[], 'fake_accuracy':[]}

	Â  Â  for epoch in range(epochs):
	Â  Â  Â  Â  epoch_start = time.time()
	Â  Â  Â  Â  for it, image_batch in enumerate(dataset):
	Â  Â  Â  Â  Â  Â  gen_loss, disc_loss, real_accuracy, fake_accuracy = train_step(image_batch)
	Â  Â  Â  Â  Â  Â  history['gen_loss'].append(gen_loss)
	Â  Â  Â  Â  Â  Â  history['disc_loss'].append(disc_loss)
	Â  Â  Â  Â  Â  Â  history['real_accuracy'].append(real_accuracy)
	Â  Â  Â  Â  Â  Â  history['fake_accuracy'].append(fake_accuracy)

	Â  Â  Â  Â  Â  Â  if it % 50 == 0:
	Â  Â  Â  Â  Â  Â  Â  Â  display.clear_output(wait=True)
	Â  Â  Â  Â  Â  Â  Â  Â  generate_and_save_images(generator, epoch+1, it+1, seed)
	Â  Â  Â  Â  Â  Â  Â  Â  print('Epoch {} | iter {}'.format(epoch+1, it+1))
	Â  Â  Â  Â  Â  Â  Â  Â  print('Time for epoch {} : {} sec'.format(epoch+1, int(time.time()-epoch_start)))

	Â  Â  Â  Â  if (epoch + 1) % save_every == 0:
	Â  Â  Â  Â  Â  Â  checkpoint.save(file_prefix=checkpoint_prefix)

	Â  Â  Â  Â  display.clear_output(wait=True)
	Â  Â  Â  Â  generate_and_save_images(generator, epochs, it, seed)
	Â  Â  Â  Â  print('Time for training : {} sec'.format(int(time.time()-start)))
	

	Â  Â  Â  Â  draw_train_history(history, epoch)
	```

	```python
	save_every = 5
	EPOCHS = 100

	# ì‚¬ìš©ê°€ëŠ¥í•œ GPU ë””ë°”ì´ìŠ¤ í™•ì¸
	tf.config.list_physical_devices("GPU")
	```

	```python
	%%time
	train(seed, EPOCHS, save_every)

	# í•™ìŠµê³¼ì •ì˜ loss, accuracy ê·¸ë˜í”„ ì´ë¯¸ì§€ íŒŒì¼ì´ ~/training_history ê²½ë¡œì— ìƒì„±ë˜ê³  ìˆìœ¼ë‹ˆ
	# ì§„í–‰ ê³¼ì •ì„ ìˆ˜ì‹œë¡œ í™•ì¸í•´ ë³´ì‹œê¸¸ ê¶Œí•©ë‹ˆë‹¤.
	```

#### (8) í•™ìŠµê³¼ì • ì‹œê°í™”
```python
anim_file = 'fashion_mnist_dcgan.gif'
with imageio.get_writer(anim_file, mode='I') as writer:
Â  Â  filenames = glob.glob('generated_samples/sample*.png')
Â  Â  filenames = sorted(filenames)
Â  Â  last = -1
Â  Â  for i, filename in enumerate(filenames):
Â  Â  Â  Â  frame = 2*(i**0.5)
Â  Â  Â  Â  if round(frame) > round(last):
Â  Â  Â  Â  Â  Â  last = frame
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â  image = imageio.imread(filename)
Â  Â  Â  Â  writer.append_data(image)
Â  Â  image = imageio.imread(filename)
Â  Â  writer.append_data(image)
# fashion/fashion_mnist_dcgan.gif
```


#### âœ¨ ì£¼ìš” ì„±ëŠ¥ ê°œì„  ë°©ì‹
---
- ì°¸ê³ ë§í¬ :Â [https://github.com/soumith/ganhacks](https://github.com/soumith/ganhacks)
	1. Normalize the inputs : normalize the images between -1 and 1 â˜‘ï¸
	2. A modified loss function : Flip labels when training generator: real = fake, fake = real â˜‘ï¸
	3. Use a spherical Z : Sample from a gaussian distribution â˜‘ï¸
	4. BatchNorm : Construct different mini-batches for real and fake â˜‘ï¸
	5. Avoid Sparse Gradients: ReLU, MaxPool â˜‘ï¸
	6. Use Soft and Noisy Labels : if it is real, then replace the label with a random number between 0.7 and 1.2, and if it is a fake sample, replace it with 0.0 and 0.3
	7. DCGAN / Hybrid Models â˜‘ï¸
	8. Use stability tricks from RL : All stability tricks that work for deep deterministic policy gradients
	9. Use the ADAM Optimizer â˜‘ï¸
	10. Track failures early â˜‘ï¸
	11. Dont balance loss via statistics (unless you have a good reason to) â˜‘ï¸
	12. If you have labels, use them
	13. Add noise to inputs, decay over time
	14. `notsure` Train discriminator more (sometimes)
	15. `notsure` Batch Discrimination
	16. Discrete variables in Conditional GANs : Use an Embedding layer, Add as additional channels to images
	17. Use Dropouts in G in both train and test phase : Provide noise in the form of dropout (50%).

### âœ¨ ê´€ë ¨ í”„ë¡œì íŠ¸ ë§í¬
---
- [github_file_link](https://github.com/NeatyNut/AIFFEL_Online_Quest/blob/main/Exploration/Ex04/Exploration_CR7_%EC%83%9D%EC%84%B1%ED%98%95AI.ipynb)